# -*- coding: utf-8 -*-
"""DayanneMaldonado_TDAH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oe9CQ5mN3swgmsbMRrXnGeP6pOzz0DX6

# Nilearn
Instalamos la librería que nos permite realizar análisis accesibles y versátiles de los volúmenes cerebrales. Proporciona herramientas estadísticas y de aprendizaje automático, con documentación instructiva. Tiene una variedad de conjuntos de datos preprocesados ​​que podemos descargar fácilmente con una función integrada.
"""

# Commented out IPython magic to ensure Python compatibility.
# Verificamos si nilearn está instalado
try:
    import nilearn
except ImportError:
    # sino, instalamos la librería
#     %pip install nilearn

"""# Dataset
Importamos la librería, y usaremos el conjunto de datos de acceso público ADHD-200 compuesto por IRMf(Imagen por resonancia magnética funcional) en estado de reposo y datos anatómicos.
[ADHD200](http://fcon_1000.projects.nitrc.org/indi/adhd200/)
"""

from nilearn import datasets
num = 40
tdah_datos = datasets.fetch_adhd(n_subjects = num)

"""# Keys
Usamos .keys() para inspeccionar el conjunto de datos, existen 4 tipos de atributos:



1.   "func" presenta las rutas a las imágenes de datos rs-fMRI
2.   "confounds" son los archivos CSV que contienen variables
3.   “phenotypic” proporciona explicaciones para los pasos de preprocesamiento
4.   “description” es una descripción del conjunto de datos

"""

tdah_datos.keys()

import nibabel as nib

img = nib.load(tdah_datos.func[0])
img.shape

"""Ahora, esto nos dice que para cada paciente hay 176 volúmenes, cada uno con una estructura 3D de (61,73,61)"""

print(img)

!pip install --upgrade matplotlib

import matplotlib.pyplot as plt
from nilearn import image
from nilearn import plotting

imagen_xyz = image.mean_img(tdah_datos.func[0])
plotting.view_img(imagen_xyz, threshold = None,colorbar = False)

"""# Objetivo

Este conjunto de datos presenta tanto a individuos bajo ("controles") como a aquellos con TDAH diagnosticado ("tratamientos").

Extraeremos los coeficientes de conectividad funcional de este conjunto de datos para clasificar si un sujeto determinado es un "control" o un "tratamiento".

Lo haremos usando primero un Análisis de componentes independientes (ICA) de nilearn y luego lo usaremos para extraer coeficientes de conectividad funcional. Finalmente, construiremos una red neuronal utilizando estos coeficientes para clasificar "controles" de "tratamientos".

#Elección de regiones de interés

# Parcelación con MSDL Atlas
visualizaremos el atlas MSDL (aprendizaje de diccionario de múltiples temas), que define un conjunto de ROI probabilísticos en todo el cerebro.
"""

import numpy as np

msdl_atlas = datasets.fetch_atlas_msdl()

msdl_coordenadas = msdl_atlas.region_coords
n_regiones = len(msdl_coordenadas)

print('MSDL tiene {} ROIs, parte de las siguientes redes : \n {}'.format(n_regiones, np.unique(msdl_atlas.networks)))

"""Entonces, estos son las reguiones de interés según el atlas MSDL."""

plotting.plot_prob_atlas(msdl_atlas.maps)

from nilearn import input_data
masker_msdl = input_data.NiftiMapsMasker(msdl_atlas.maps,resampling_target="data", detrend = True,
                                      standardize=True,smoothing_fwhm=6,memory='nilearn_cache',memory_level=1).fit()

sujetos_msdl = []
for func_file, confound_file in zip(
tdah_datos.func, tdah_datos.confounds):
  series_de_tiempo_msdl = masker_msdl.transform(func_file, confounds=confound_file)
  sujetos_msdl.append(series_de_tiempo_msdl)

from nilearn import input_data
masker_msdl = input_data.NiftiMapsMasker(msdl_atlas.maps,resampling_target="data", detrend = True,
                                      standardize=True,smoothing_fwhm=6,memory='nilearn_cache',memory_level=1).fit()
seriest = masker_msdl.transform(tdah_datos.func[0])
print(f"Serie de tiempo con la siguiente forma: {seriest}")

"""# Parcelación usando BSC(bootstrap analysis of stable clusters)"""

basc = datasets.fetch_atlas_basc_multiscale_2015()

plotting.plot_roi(basc["scale444"],title = "BASC atlas",colorbar = True, cmap = "nipy_spectral")

masker_basc = input_data.NiftiLabelsMasker(basc['scale444'],resampling_target = "data",detrend = True,
                                      standardize=True,smoothing_fwhm=5,memory='nilearn_cache',memory_level=1).fit()
sujetos_basc = []
for func_file, confound_file in zip(
  tdah_datos.func, tdah_datos.confounds):
  series_de_tiempo_basc = masker_basc.transform(func_file, confounds=confound_file)
  sujetos_basc.append(series_de_tiempo_basc)

print(f"Serie de tiempo con la siguiente forma:{sujetos_basc[0].shape} (#Puntos en el tiempo,#Parcelas)")

"""# Agrupación jerárquica

"""

from nilearn.connectome import ConnectivityMeasure
from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree

conn_basc = ConnectivityMeasure(kind='correlation').fit(sujetos_basc)
sucesor = linkage(conn_basc.mean_, method='average', metric='euclidean')

part_basc = np.squeeze(cut_tree(sucesor,n_clusters=100))
part_basc_img = masker_basc.inverse_transform(part_basc.reshape([1,444])+1)

plotting.plot_roi(part_basc_img,title='Grupo de redes',colorbar=True,cmap='Paired')

"""Parcelación"""

masker_basc_cluster = input_data.NiftiLabelsMasker(part_basc_img, smoothing_fwhm=6,
 standardize=False, detrend=True,
 t_r=2.5, low_pass=0.1,
 high_pass=0.01).fit()

sujetos_basc_cluster = []
for func_file, confound_file in zip(
  tdah_datos.func, tdah_datos.confounds):
  series_de_tiempo_basc_cluster = masker_basc_cluster.transform(func_file, confounds=confound_file)
  sujetos_basc_cluster.append(series_de_tiempo_basc_cluster)

print(f"Serie de tiempo con la siguiente forma: {sujetos_basc_cluster[0].shape} (#Puntos en el tiempo,#Parcelas)")

"""# Parcelación agrupado pabellones cerebrales"""

from nilearn.regions import Parcellations

ward = Parcellations(method='ward', n_parcels=400,
                     standardize=False, smoothing_fwhm=10.,
                     memory='nilearn_cache', memory_level=1,
                     verbose=1)
ward.fit(tdah_datos.func)

from nilearn.image import mean_img

etiquetas_ward_img = ward.labels_img_

mean_func_img = mean_img(tdah_datos.func[0])
plotting.plot_roi(etiquetas_ward_img, mean_func_img,title="Parcelación por Pabellones",cut_coords=(2,-25,13),colorbar=True,cmap='Paired')

masker_ward = input_data.NiftiLabelsMasker(etiquetas_ward_img, smoothing_fwhm=6,
 standardize=False, detrend=True,
 t_r=2.5, low_pass=0.1,
 high_pass=0.01).fit()

sujetos_ward = []
for func_file, confound_file in zip(
  tdah_datos.func, tdah_datos.confounds):
  series_de_tiempo_ward = masker_ward.transform(func_file, confounds=confound_file)
  sujetos_ward.append(series_de_tiempo_ward)

print(f"Serie de tiempo con la siguiente forma: {sujetos_ward[0].shape} (#Puntos en el tiempo,#Parcelas)")

"""#Análisis de componentes independientes (ICA)
Lo utilizamos para evaluar la conectividad funcional. Nilearn tiene un método para ICA a nivel de grupo (CanICA) que permite controlar la variabilidad de un solo sujeto, especialmente dado que estamos interesados ​​en redes funcionales.

Elegimos usar una descomposición de 20 componentes(basada en los estándares proporcionados en la documentación de Nilearn para el conjunto de datos elegido).
"""

from nilearn import decomposition

canica = decomposition.CanICA(n_components=20,mask_strategy = 'background')
canica.fit(tdah_datos.func)

#Recuperamos los componentes
componentes = canica.components_

#Utilizamos masker para obtener los componentes independientes
componentes_img = canica.masker_.inverse_transform(componentes)

"""#Masker
Utilizamos ICA como un filtro para extraer las regiones que nos interesan. Lo hacemos llamando a la función NiftiMapsMasker de Nilearn para "resumir" las señales cerebrales que obtuvimos usando ICA. Una vez que tenemos eso, transformamos los datos extraídos en series de tiempo usando el método fit_transform.
Luego usamos todo lo que sabemos sobre el conjunto de datos (archivos "función", "confundidos" y "fenotípicos") para obtener la información que necesitamos, incluso si el sujeto es "tratamiento" o "control" y su ubicación de recopilación de datos asociada ( sitio).
Contabilización de fuentes de ruido
Los enmascaradores nos permiten realizar otras operaciones útiles además de crear nuestros datos.
Un paso importante de preprocesamiento es la corrección de las señales medidas sin interés (por ejemplo, el movimiento de la cabeza). Nuestro als adhd_data incluye varias de estas señales sin interés. Podemos acceder a estos usando el atributo de confusión, usando adhd_data.confounds.
"""

import pandas as pd

pd.read_table(tdah_datos.confounds[0]).head()

#Uso de un filtro para extraer las series temporales de las regiones
from nilearn import input_data

masker_canica = input_data.NiftiMapsMasker(componentes_img, smoothing_fwhm=6,
 standardize=False, detrend=True,
 t_r=2.5, low_pass=0.1,
 high_pass=0.01)


#Cómputo de las señales de las regiones y extracción de la información fenotípica de interés
sujetos_canica = []
sitios = []
etiquetas = []

#transformamos los datos extraídos en series de tiempo usando el método fit_transform. Luego usamos todo lo que sabemos sobre el conjunto de datos
#(de los archivos “func”, “confounds”, and “phenotypic” files) para obtener la información que necesitamos, incluso si el sujeto es "tratamiento"
#o "control" y su ubicación de recopilación de datos asociada( sitio).

for func_file, confound_file, phenotypic in zip(
  tdah_datos.func, tdah_datos.confounds, tdah_datos.phenotypic):
  series_de_tiempo_canica = masker_canica.fit_transform(func_file, confounds=confound_file)
  sujetos_canica.append(series_de_tiempo_canica)
  sitios.append(phenotypic['site'])
  etiquetas.append(phenotypic['adhd'])

print(len(sujetos_canica))
sujetos_canica[0].shape

"""Ahora, aquí podemos ver que tenemos 20 ROI

# Conectomas
Ahora, necesitamos obtener la conectividad funcional entre las regiones de interés que extrajimos. El tipo de conectividad funcional más simple y más utilizado es la correlación por pares entre ROI.
"""

from nilearn.connectome import ConnectivityMeasure

medida_correlacion = ConnectivityMeasure(kind='correlation')
correlacion_matrices_msdl = medida_correlacion.fit_transform(sujetos_msdl)
correlacion_matrices_basc = medida_correlacion.fit_transform(sujetos_basc_cluster)
correlacion_matrices_ward = medida_correlacion.fit_transform(sujetos_ward)
correlacion_matrices_canica = medida_correlacion.fit_transform(sujetos_canica)

print(correlacion_matrices_msdl.shape)
print(correlacion_matrices_basc.shape)
print(correlacion_matrices_ward.shape)
print(correlacion_matrices_canica.shape)

#Separación de las matrices en las de los sujetos de tratamiento frente a los de control para la comparación.
#Visualicemos la diferencia en el conectoma de aquellos en tratamiento o sujetos de control.
tdah_correlaciones_msdl = []
control_correlaciones_msdl = []

tdah_correlaciones_basc = []
control_correlaciones_basc = []

tdah_correlaciones_ward = []
control_correlaciones_ward = []

tdah_correlaciones_canica = []
control_correlaciones_canica = []

for i in range(40):
    if etiquetas[i] == 1:
        tdah_correlaciones_msdl.append(correlacion_matrices_msdl[i])
        tdah_correlaciones_basc.append(correlacion_matrices_basc[i])
        tdah_correlaciones_ward.append(correlacion_matrices_ward[i])
        tdah_correlaciones_canica.append(correlacion_matrices_canica[i])
    else:
        control_correlaciones_msdl.append(correlacion_matrices_msdl[i])
        control_correlaciones_basc.append(correlacion_matrices_basc[i])
        control_correlaciones_ward.append(correlacion_matrices_ward[i])
        control_correlaciones_canica.append(correlacion_matrices_canica[i])

#Obtener la matriz de correlación media en todos los sujetos de tratamiento
correlacion_media_msdl_tdah = np.mean(tdah_correlaciones_msdl, axis=0).reshape(series_de_tiempo_msdl.shape[-1],
                                                          series_de_tiempo_msdl.shape[-1])
correlacion_media_basc_tdah = np.mean(tdah_correlaciones_basc, axis=0).reshape(series_de_tiempo_basc_cluster.shape[-1],
                                                          series_de_tiempo_basc_cluster.shape[-1])
correlacion_media_ward_tdah = np.mean(tdah_correlaciones_ward, axis=0).reshape(series_de_tiempo_ward.shape[-1],
                                                          series_de_tiempo_ward.shape[-1])
correlacion_media_canica_tdah = np.mean(tdah_correlaciones_canica, axis=0).reshape(series_de_tiempo_canica.shape[-1],
                                                          series_de_tiempo_canica.shape[-1])

#Obtener la matriz de correlación media en todos los sujetos de control
correlacion_media_msdl_control = np.mean(control_correlaciones_msdl, axis=0).reshape(series_de_tiempo_msdl.shape[-1],
                                                          series_de_tiempo_msdl.shape[-1])
correlacion_media_basc_control = np.mean(control_correlaciones_basc, axis=0).reshape(series_de_tiempo_basc_cluster.shape[-1],
                                                          series_de_tiempo_basc_cluster.shape[-1])
correlacion_media_ward = np.mean(control_correlaciones_ward, axis=0).reshape(series_de_tiempo_ward.shape[-1],
                                                          series_de_tiempo_ward.shape[-1])
correlacion_media_canica_control = np.mean(control_correlaciones_canica, axis=0).reshape(series_de_tiempo_canica.shape[-1],
                                                          series_de_tiempo_canica.shape[-1])

"""#Calsificación NN
Ahora que tenemos las matrices de correlación que proporcionan una medida vectorizada de conectividad funcional, podemos usarlas como datos de entrada para nuestra red neuronal.
"""

conectividad = ConnectivityMeasure(kind = 'correlation',vectorize = True)
conectomas_msdl = conectividad.fit_transform(sujetos_msdl)
conectomas_basc = conectividad.fit_transform(sujetos_basc)
conectomas_ward = conectividad.fit_transform(sujetos_ward)
conectomas_canica = conectividad.fit_transform(sujetos_canica)
print(conectomas_msdl.shape)
print(conectomas_basc.shape)
print(conectomas_ward.shape)
print(conectomas_canica.shape)

from sklearn.decomposition import PCA

pca = PCA(n_components = 0.9)
pca.fit(conectomas_msdl)
conectomas_msdl = pca.transform(conectomas_msdl)

pca.fit(conectomas_basc)
conectomas_basc = pca.transform(conectomas_basc)

pca.fit(conectomas_ward)
conectomas_ward = pca.transform(conectomas_ward)

pca.fit(conectomas_canica)
conectomas_canica = pca.transform(conectomas_canica)

print(conectomas_msdl.shape)
print(conectomas_basc.shape)
print(conectomas_ward.shape)
print(conectomas_canica.shape)

#Dividimos los datos

from sklearn.model_selection import train_test_split
X_train_msdl, X_test_msdl, y_train_msdl, y_test_msdl = train_test_split(conectomas_msdl, etiquetas, test_size=0.15)
X_train_basc, X_test_basc, y_train_basc, y_test_basc = train_test_split(conectomas_basc, etiquetas, test_size=0.15)
X_train_ward, X_test_ward, y_train_ward, y_test_ward = train_test_split(conectomas_ward, etiquetas, test_size=0.15)
X_train_canica, X_test_canica, y_train_canica, y_test_canica = train_test_split(conectomas_canica, etiquetas, test_size=0.15)

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

clasificador = Sequential()

#Primera capa oculta
clasificador.add(Dense(32, activation='tanh', kernel_initializer='random_normal', input_shape=conectomas_msdl.shape[1:]))
#Segunda capa oculta
clasificador.add(Dense(16, activation='relu', kernel_initializer='random_normal'))
#Tercera capa oculta
clasificador.add(Dense(8, activation='relu', kernel_initializer='random_normal'))
#Capa de salida
clasificador.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))
#Compilamos el modelo
clasificador.compile(optimizer = Adam(lr =.0005),loss='binary_crossentropy', metrics =['accuracy'])
#Entreamos el modelo
clasificador.fit(np.array(X_train_msdl),np.array(y_train_msdl), batch_size=8, epochs=100)

eval_modelo=clasificador.evaluate(np.array(X_train_msdl), np.array(y_train_msdl))
eval_modelo

y_pred_msdl=clasificador.predict(X_test_msdl,batch_size=16)
y_pred_msdl =(y_pred_msdl>0.5)

from sklearn.metrics import confusion_matrix, classification_report

mc = confusion_matrix(y_test_msdl, y_pred_msdl)
print(mc)
rp = classification_report(y_test_msdl, y_pred_msdl)
print(rp)

clasificador = Sequential()

#Primera capa oculta
clasificador.add(Dense(32, activation='tanh', kernel_initializer='random_normal', input_shape=conectomas_basc.shape[1:]))
#Segunda capa oculta
clasificador.add(Dense(16, activation='relu', kernel_initializer='random_normal'))
#Tercera capa oculta
clasificador.add(Dense(8, activation='relu', kernel_initializer='random_normal'))
#Capa de salida
clasificador.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))
#Compilamos el modelo
clasificador.compile(optimizer = Adam(lr =.0005),loss='binary_crossentropy', metrics =['accuracy'])
#Entrenamos el modelo
clasificador.fit(np.array(X_train_basc),np.array(y_train_basc), batch_size=8, epochs=75)

eval_modelo=clasificador.evaluate(np.array(X_train_basc), np.array(y_train_basc))
eval_modelo

y_pred_basc=clasificador.predict(X_test_basc,batch_size=16)
y_pred_basc =(y_pred_basc>0.5)

from sklearn.metrics import confusion_matrix, classification_report

mc = confusion_matrix(y_test_basc, y_pred_basc)
print(mc)
rp = classification_report(y_test_basc, y_pred_basc)
print(rp)

clasificador = Sequential()

#Primera capa oculta
clasificador.add(Dense(32, activation='tanh', kernel_initializer='random_normal', input_shape=conectomas_ward.shape[1:]))
#Segunda capa oculta
clasificador.add(Dense(16, activation='relu', kernel_initializer='random_normal'))
#Tercera capa oculta
clasificador.add(Dense(16, activation='relu', kernel_initializer='random_normal'))
#Capa de salida
clasificador.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))
#Compilamos el modelo
clasificador.compile(optimizer = Adam(lr =.0001),loss='binary_crossentropy', metrics =['accuracy'])
#Entrenamos el modelo
clasificador.fit(np.array(X_train_ward),np.array(y_train_ward), batch_size=8, epochs=75)

eval_modelo=clasificador.evaluate(np.array(X_train_ward), np.array(y_train_ward))
eval_modelo

y_pred_ward=clasificador.predict(X_test_ward,batch_size=8)
y_pred_ward =(y_pred_ward>0.45)

from sklearn.metrics import confusion_matrix, classification_report

mc = confusion_matrix(y_test_ward, y_pred_ward)
print(mc)
rp = classification_report(y_test_ward, y_pred_ward)
print(rp)

clasificador = Sequential()

#Primera capa oculta
clasificador.add(Dense(32, activation='tanh', kernel_initializer='random_normal', input_shape=conectomas_canica.shape[1:]))
#Segunda capa oculta
clasificador.add(Dense(16, activation='relu', kernel_initializer='random_normal'))
#Tercera capa oculta
clasificador.add(Dense(8, activation='relu', kernel_initializer='random_normal'))
#Capa de salida
clasificador.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))
#Compilamos el modelo
clasificador.compile(optimizer = Adam(lr =.0005),loss='binary_crossentropy', metrics =['accuracy'])
#Entrenamos el modelo
clasificador.fit(np.array(X_train_canica),np.array(y_train_canica), batch_size=16, epochs=75)

eval_modelo=clasificador.evaluate(np.array(X_train_canica), np.array(y_train_canica))
eval_modelo

y_pred_canica=clasificador.predict(X_test_canica)
y_pred_canica =(y_pred_canica>0.5)

from sklearn.metrics import confusion_matrix, classification_report

mc = confusion_matrix(y_test_canica, y_pred_canica)
print(mc)
rp = classification_report(y_test_canica, y_pred_canica)
print(rp)